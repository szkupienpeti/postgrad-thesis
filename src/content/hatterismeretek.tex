\chapter{Elméleti áttekintés} \label{ch:hatterismeretek}

Hosszú út vezetett az első szoftverek megjelenésétől a generatív mesterséges intelligencia térhódításáig. Az elmúlt évtizedekben a szoftverfejlesztés területén drámai változások zajlottak le, amelyek alapjaiban formálták át a szakmát és a fejlesztési folyamatokat. A technológiai fejlődés következtében egyre összetettebb rendszerek épültek ki, amelyek kezelése és fejlesztése új megközelítéseket és eszközöket igényelt.

Ebben a fejezetben összefoglalom a szakdolgozat témájához kapcsolódó elméleti ismereteket. A \ref{sec:szoftverfejlesztes}. alfejezetben áttekintem a szoftverfejlesztési folyamat lépéseit és fontosabb módszertanait. A \ref{sec:mi}. alfejezetben bemutatom a mesterséges intelligencia főbb területeit. Végül a \ref{sec:mi-fejl}. alfejezetben ismertetem, hogyan hat a mesterséges intelligencia a szoftverfejlesztésre.

\section{Szoftverfejlesztés}\label{sec:szoftverfejlesztes}

A számítógépek fejlődésével egyre bonyolultabb problémák kerültek a programozók látókörébe, hiszen a növekvő számítási kapacitás egyre több esetben volt már elegendő. Ez a potenciál nem is maradt kihasználatlanul, ami a szoftverrendszerek komplexitásának drámai növekedéséhez vezetett \parencite{Briggs02042020}. A kezdeti komplexitást jól mutatja, hogy az első elektronikusan tárolt program, amit Tom Kilburn írt 1948-ban egy szám legnagyobb valódi osztójának megkeresésére, mindössze 17 utasításból állt \parencite{lavington1998history}. Ezzel szemben a Google összes szoftverének együttes kódbázisát 2 milliárd sorosra becsülik \parencite{googlecodebase}, ami jól mutatja a robbanásszerű fejlődést.

A kevesebb, mint egy évszázad alatt ilyen meredeken növekvő bonyolultságot látva nem szabad azonban szem elől tévesztenünk, hogy az emberi agy kapacitása nem változott. Vagyis a szoftveresen megoldott problémák komplexitása bőven átlépte már azt a határt, amit egy ember még részleteiben képes átlátni. Ennek a kezelését az újabb és újabb absztrakciós szintek bevezetése tette lehetővé, hiszen a magasabb absztrakciós szinteken már nem nehezítik a tisztánlátást az alacsonyabb szintek részletei. A növekvő komplexitás, és az abból következő különböző absztrakciós szintek összhangban tartása szükségessé tette strukturált fejlesztési módszertanok kidolgozását, amelyek segítségével a projektek kézben tarthatók és a csapatok hatékonyan tudnak együttműködni.

\subsection{Szoftverfejlesztési életciklus}\label{subsec:sdlc}

A szoftverfejlesztés során már a korai évtizedekben nyilvánvalóvá vált, hogy a növekvő komplexitás és a hatékony csapatmunka strukturált megközelítést igényel. A kezdeti spontán, ad hoc fejlesztés gyakran vezetett időbeli csúszásokhoz, költségtúllépéshez és minőségi problémákhoz. Ezek a nehézségek hívták életre a szoftverfejlesztés első modelljét (vízesésmodell), amely strukturált fázisokra bontotta a folyamatot \parencite{Royce1970ManagingDevelopment}. Később ez alapján dolgozták ki a \emph{szoftverfejlesztési életciklus} (Software Development Life Cycle, SDLC) koncepcióját, amely általános keretrendszert ad a szoftverek tervezéséhez, fejlesztéséhez, teszteléséhez és karbantartásához \parencite{Boehm1988Spiral, Sommerville2015Software}. Az SDLC célja, hogy a fejlesztés folyamata átlátható, megismételhető, hatékony és mérhető legyen, hozzájárulva ezzel ahhoz, hogy az elkészült termék végül megfeleljen a megrendelői és felhasználói elvárásoknak.

A szoftverfejlesztési életciklus modellje tehát nemcsak technikai iránytű, hanem menedzsment eszköz is: közös nyelvet biztosít a szoftverfejlesztés folyamatához, elősegítve a kommunikációt a különböző szerepkörök között, támogatja a tervezést és a minőségbiztosítást, valamint csökkenti a projektkockázatokat \parencite{sdlc4ispm}. A jól definiált fázisok segítenek abban, hogy a fejlesztési folyamat logikusan épüljön fel, és minden lépésnek világos bemenetei és kimenetei legyenek. Bár az egyes szervezetek és módszertanok eltérően valósítják meg, az SDLC alapvetően a következő lépésekből áll \parencite{IBM2025SDLC}.

\begin{enumerate}
	\item \textbf{Projekttervezés (Planning).} Célja a projekt céljainak, hatókörének, erőforrásigényének és kockázatainak meghatározása. Ebben a szakaszban történik a projekt ütemezése és a kezdeti költségbecslés is, ami alapot ad a további fejlesztési döntésekhez. Eredménye a kezdeti szoftverkövetelmény specifikáció (Software Requirement Specification, SRS).
	
	\item \textbf{Elemzés (Analysis).} A fejlesztendő rendszer funkcionális és extrafunkcionális (nem funkcionális) követelményeinek összegyűjtése, elemzése és dokumentálása. A cél, hogy minden érintett fél számára egyértelmű legyen, mit kell a rendszernek teljesítenie. Eredménye a követelmények részletes dokumentációja.
	
	\item \textbf{Rendszertervezés (Design).} A rendszer logikai és technikai architektúrájának kialakítása, beleértve az adatmodelleket, a komponensek közötti kapcsolatokat, interfészeket és a felhasználói felület alapvető struktúráját. Az átgondolt tervezés biztosítja, hogy az implementáció során már egyértelmű legyen, mit is kell csinálni. Eredménye a szoftverterv dokumentáció (Software Design Document, SDD).
	
	\item \textbf{Fejlesztés (Development).} A szoftver tényleges megvalósítása (implementálása) a korábbi fázisok során keletkezett dokumentumok alapján, vagyis a forráskód elkészítése, a komponensek integrálása és bizonyos előzetes egységtesztek végrehajtása. Eredménye a szoftver egy funkcionális (működő) prototípusa.
	
	\item \textbf{Tesztelés (Testing).} A fejlesztett rendszer validálása, amely során ellenőrzik, hogy az a tervezett követelményeknek megfelelően működik-e. Számos különböző módszer szolgál a hibák azonosítására, mint pl. statikus kódanalízis, code review, különböző manuális/automata tesztek (egységteszt, integrációs teszt, rendszerteszt), sérülékenységvizsgálat. A hibák azonosítása és dokumentálása után természetesen a javításuk következik, egészen addig, amíg az újratesztelés sikerrel nem jár. Eredménye egy javított, jobb minőségű (ideális esetben akár hibamentes) szoftver.
	
	\item \textbf{Bevezetés (Deployment).} A kész rendszer éles környezetbe helyezése, ahol már hozzáférnek a tényleges végfelhasználók. A technikai bevezetésen túl ide tartozik annak a biztosítása is, hogy a felhasználók valóban értsék, hogyan kell használniuk az új rendszert, illetve, hogy a bevezetés a lehető legkevésbé akassza meg a meglévő folyamatokat. Eredménye egy olyan szoftver, ami már a cégfelhasználók számára is elérhető.
	
	\item \textbf{Karbantartás (Maintenance).} A rendszer hosszú távú támogatása garantálja a szoftver folyamatos működőképességét és alkalmazkodását a változó üzleti igényekhez. Ez magában foglalja frissítések és hibajavítások biztosítását, valamint akár új funkciók fejlesztését is. Eredménye egy frissebb, javított szoftver.
\end{enumerate}

A fázisok egymásra épülnek, a különböző fejlesztési modellek azonban már eltérően értelmezhetik a fázisok közti átmeneteket. Míg a legegyszerűbb megközelítés szerint a fázisok lineárisan követik egymást, más modellek szerint iteratívan is végrehajthatók. Látható tehát, hogy az SDLC csupán testre szabható közös alapot teremt a számos különböző modell számára, amelyek így egységesen elemezhetők és összehasonlíthatók \parencite{sdlcmethods}.

\subsection{Klasszikus modellek}

A szoftverfejlesztés első modellje a \textit{vízesésmodell} (\ref{fig:vizeses}. ábra, \cite{Royce1970ManagingDevelopment}), amiben a diszjunkt fázisok szigorúan szekvenciálisan követik egymást. Ez a modell egyszerű és átlátható, a fázisok közti átmenetek, valamint a be- és kimenetek egyértelműek. Fontos azonban megjegyezni, hogy a vízesésmodell nem tudja hatékonyan kezelni a követelmények utólagos változását, hiszen ekkor elölről kellene kezdeni az egész folyamatot.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth, keepaspectratio]{figures/vizeses.png}
	\caption{A vízesésmodell lépései. \parencite{Royce1970ManagingDevelopment}}
	\label{fig:vizeses}
\end{figure}

Noha a vízesésmodellt tekintjük az első formálisan leírt modellnek, az \textit{iteratív} megközelítés már ennél korábban is megfogalmazódott (\cite{iterative}). Ebben a szemléletben a fázisok között visszacsatolás van, vagyis a rendszer több iteráció során válik egyre részletesebbé, míg el nem nyeri végleges formáját. (Valójában \textcite{Royce1970ManagingDevelopment} már a vízesésmodellt bemutató cikkében is írt visszacsatolásról (\textit{,,do it twice''}), de ez a modell mégis szigorúan szekvenciálissá egyszerűsítve terjedt el.)

A szekvenciális és iteratív megközelítések ötvözéséből született meg a \textit{spirálmodell}, amely iteratív ciklusokban dolgozik és külön hangsúlyt fektet a kockázatelemzésre minden egyes fázisban (\ref{fig:spiral}. ábra, \cite{Boehm1988Spiral}). A spirálban minden „gyűrű” egy újabb fejlesztési ciklust jelöl, amiben célokat határoznak meg, értékelik a rizikófaktorokat, prototípusokat fejlesztenek, majd megtervezik a következő ciklust. Ez a modell különösen olyan összetett projektekben alkalmazható, ahol a követelmények gyakran változnak, vagy magas a kockázat.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth, keepaspectratio]{figures/spiral.png}
	\caption{A spirálmodell lépései. \parencite{Boehm1988Spiral}}
	\label{fig:spiral}
\end{figure}

A vízesésmodell továbbfejlesztése a \textit{V-modell} (\ref{fig:vizeses}. ábra, \cite{Rook1986VModel}), amelyben különös hangsúlyt kap a tesztelés, hiszen minden fejlesztési szinthez kapcsolódik egy tesztelési fázis is. A V bal szára az egyre részletesebb tervezési lépésekből áll (top-down), amiket alul a tényleges implementáció követ. A V jobb szára pedig az egyre magasabb szintű validációs fázisokat tartalmazza (bottom-up). Előnye, hogy az egyes fejlesztési lépésekkel párhuzamosan tervezhetők a kapcsolódó tesztek, ugyanakkor nem elég rugalmas ahhoz, hogy kezelni tudja a változó követelményeket.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth, keepaspectratio]{figures/vmodel.png}
	\caption{A V-modell lépései. (\textcite{Boehm1988Spiral} ábrája feljavított minőségben, saját szerkesztés.)}
	\label{fig:vmodel}
\end{figure}

\subsection{Agilis módszertanok}\label{subsec:agilis}

A klasszikus, szekvenciális fejlesztési modellek (például a vízesés- és a V-modell) jól strukturáltak, de a gyakorlatban gyakran bizonyultak túlságosan merevnek a gyorsan változó üzleti és technológiai környezetben. A követelmények ritkán maradnak változatlanok egy teljes projektidőszakon át, és a felhasználói igények sokszor csak a fejlesztés előrehaladtával tisztulnak ki. Ezt a problémát a korai modellek nehezen tudták kezelni, mivel a folyamat lineáris jellege miatt egy későbbi fázisban felmerülő változás az egész fejlesztési ciklust visszavethette. Ebből a felismerésből született meg az \emph{agilis szoftverfejlesztés} gondolata, amely a rugalmasságot, az iterativitást és a folyamatos visszacsatolást helyezi a középpontba \parencite{Beck2001AgileManifesto,Highsmith2002Agile}.

Az agilis szemlélet 2001-ben vált formálisan meghatározottá, amikor 17 szoftverfejlesztő megfogalmazta az \emph{Agile Manifestót} (\textit{Manifesto for Agile Software Development}, \cite{Beck2001AgileManifesto}). A dokumentum négy alapértéket és tizenkét elvet rögzít, amelyek célja a fejlesztés emberközpontúbbá, együttműködőbbé és gyorsabban reagálóvá tétele. A négy alapérték a következő:
\begin{itemize}
	\item az \textbf{egyének és interakciók} fontosabbak, mint a folyamatok és eszközök,
	\item a \textbf{működő szoftver} fontosabb, mint az átfogó dokumentáció,
	\item a \textbf{megrendelővel való együttműködés} fontosabb, mint a szerződéses tárgyalás,
	\item a \textbf{változásra való reagálás} fontosabb, mint a terv követése.
\end{itemize}
Ezek az elvek nem a dokumentáció vagy a tervezés elhagyását jelentik, hanem azoknak az emberi tényezők és a gyors visszacsatolás mögé rendelését. Az agilitás tehát nem a folyamatok hiányát, hanem azok ésszerű minimalizálását és adaptivitását jelenti.

Az agilis módszertanok közül a legismertebb a \textit{Scrum}, amely iteratív fejlesztési ciklusokat (ún. sprinteket) alkalmaz, jellemzően 2–4 hetes időtartamban. Minden sprint végén egy működő szoftververzió (inkrementum) kerül bemutatásra, amelyet a csapat retrospektív megbeszélésen értékel, így a következő iterációban azonnal érvényesíthetők a tapasztalatok \parencite{Schwaber1997ScrumGuide}.

A \textit{Kanban} módszer ezzel szemben a feladatok folyamatos elvégzésére és a vizuális feladatkövetésre épít: a munkaelemek egy táblán haladnak végig a „to do” – „in progress” – „done” állapotokon, elősegítve az átláthatóságot és a szűk keresztmetszetek felismerését \parencite{Anderson2010Kanban}.

Az \textit{Extreme Programming} (XP) a kódminőség és a fejlesztői gyakorlatok javítását helyezi előtérbe, például páros programozással (pair programming), tesztvezérelt fejlesztéssel (Test Driven Development, TDD) valamint folyamatos integrációval és folyamatos szállítással (Continuous Integration / Continuous Delivery, CI/CD) \parencite{Beck2004XP}.

Az agilis módszertanok nem önmagukban állnak, hanem az SDLC iteratív megvalósításai: az életciklus fázisai itt nem szigorú sorrendben követik egymást, hanem folyamatosan ismétlődnek kisebb körökben. A hangsúly a folyamatos értékteremtésen, a csapat autonómiáján és a visszajelzések gyors beépítésén van. Ennek köszönhetően az agilis fejlesztés különösen jól illeszkedik a gyorsan változó üzleti környezethez és a modern technológiai ökoszisztémákhoz.

\subsection{Automatizációs trendek}\label{subsec:automatizacio}

Az agilis fejlesztés térnyerésével párhuzamosan a szoftverfejlesztésben megjelent egy új szemlélet, amely a fejlesztési és üzemeltetési tevékenységek szoros integrációjára épül: ez a \textit{DevOps}. A kifejezés a \textit{Development} és \textit{Operations} szavak összevonásából származik, és egy olyan kulturális és technológiai megközelítést takar, amely az együttműködést, az automatizációt és a folyamatos visszajelzést helyezi előtérbe \parencite{Humble2010ContinuousDelivery,Kim2016DevOpsHandbook}. A DevOps célja, hogy megszüntesse a fejlesztői és az üzemeltetési csapatok közti hagyományos szakadékot, ezáltal gyorsabb, megbízhatóbb és skálázhatóbb szoftverszállítást tegyen lehetővé.

A DevOps egyik legfontosabb alapelve a \textit{folyamatos integráció és folyamatos szállítás} (CI/CD), amely az automatizációs eszközök segítségével biztosítja, hogy a kódmódosítások rendszeresen, automatizált módon épüljenek be a központi kódbázisba, majd tesztelés után akár éles környezetbe is kerülhessenek \parencite{Fowler2006CI}. Az automatizált build- és tesztfolyamatokat gyakran olyan eszközök valósítják meg, mint a \textit{Jenkins}\footnote{\url{https://www.jenkins.io/}}, a \textit{GitLab CI/CD}\footnote{\url{https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/}} vagy a \textit{GitHub Actions}\footnote{\url{https://github.com/features/actions}}, amelyek lehetővé teszik a pipeline-ok vizuális konfigurálását, a verziókezeléssel való integrációt és a különböző környezetekbe történő automatikus telepítést.

A konténerizáció és az infrastruktúra automatizálása szintén kulcsszerepet játszanak a modern DevOps-gyakorlatban. A \textit{Docker}\footnote{\url{https://www.docker.com/}} a fejlesztők számára biztosít egységes futtatási környezetet, amely minimalizálja a „works on my machine” típusú hibákat, míg a \textit{Kubernetes}\footnote{\url{https://kubernetes.io/}} a konténerizált alkalmazások automatikus ütemezését, skálázását és monitorozását végzi el \parencite{Hightower2019Kubernetes}. Az infrastruktúra leírását kód formájában (Infrastructure as Code, IaC) olyan eszközök támogatják, mint a \textit{Terraform}\footnote{\url{https://www.terraform.io/}} és az \textit{Ansible}\footnote{\url{https://www.ansible.com/}}, amelyek deklaratív módon teszik lehetővé a rendszerek konfigurációját és újrakonstruálását \parencite{Brikman2022Terraform}. Ezek az automatizációs trendek együttesen nemcsak a fejlesztés sebességét növelik, hanem hozzájárulnak a hibák korai felismeréséhez, a rendszerek megbízhatóságának növeléséhez és a \textit{folyamatos fejlesztési ciklus} (Continuous Improvement) megvalósításához.

Összességében a DevOps és a CI/CD megközelítések az agilis elvek technológiai kiterjesztései, amelyek a gyors alkalmazkodást és a folyamatos értékteremtést támogatják. Az automatizáció ma már nem csupán kényelmi eszköz, hanem versenyképességi tényező a vállalati szoftverfejlesztésben, különösen a komplex rendszerek és a felhőalapú architektúrák korában.

\subsection{Low-code, no-code paradigma}\label{subsec:lowcode}

A szoftverfejlesztés folyamatosan az automatizáció irányába mozdul el: a DevOps- és CI/CD-megközelítések az üzemeltetés és a szállítás folyamatát egyszerűsítik, míg a legújabb trendek magát a fejlesztési munkát is igyekeznek automatizálni. Ennek egyik legfontosabb irányzata a \textit{low-code} és \textit{no-code} paradigma, amelynek célja, hogy a fejlesztők – vagy akár fejlesztői háttérrel nem rendelkező üzleti felhasználók – vizuális, deklaratív eszközök segítségével hozhassanak létre alkalmazásokat \parencite{Richardson2014LowCode}. A \textit{low-code} megközelítés még igényel bizonyos mértékű programozói tevékenységet, míg a \textit{no-code} platformok teljesen grafikus, drag-and-drop alapú környezetet biztosítanak.

A low-code platformok tipikusan előre definiált komponenseket, adatkapcsolatokat és felhasználói felületi elemeket kínálnak, amelyekből gyorsan összeállíthatók alkalmazások. Az ilyen környezetek célja a fejlesztés felgyorsítása, a hibák csökkentése és az üzleti oldalon jelentkező igények gyorsabb kielégítése. Az olyan megoldások, mint az \textit{OutSystems}\footnote{\url{https://www.outsystems.com/}}, a \textit{Mendix}\footnote{\url{https://www.mendix.com/}} vagy a \textit{Microsoft Power Apps}\footnote{\url{https://powerapps.microsoft.com/}}, jól példázzák ezt a tendenciát: mindhárom platform lehetővé teszi alkalmazások gyors prototípusának elkészítését, adatkapcsolatok beállítását, valamint integrációt külső rendszerekkel. A no-code megközelítés ezt továbbviszi azáltal, hogy fejlesztői tudás nélkül is használható környezetet kínál, ilyen például a \textit{Bubble}\footnote{\url{https://bubble.io/}} vagy a \textit{Google AppSheet}\footnote{\url{https://www.appsheet.com/}}.

A low-code és no-code rendszerek jelentősége a vállalati környezetben folyamatosan növekszik, különösen ott, ahol az informatikai osztályok kapacitása korlátozott, de az üzleti igények gyors megvalósítást követelnek. Bár ezek a platformok nem alkalmasak minden fejlesztési feladatra, hatékonyan kiegészítik a hagyományos fejlesztést: lehetővé teszik az egyszerűbb alkalmazások és belső eszközök gyors előállítását, így a fejlesztők nagyobb figyelmet fordíthatnak az összetettebb problémákra. A low-code/no-code irányzat ezért a szoftverfejlesztés demokratizálásának egyik kulcstényezője, és előkészíti a terepet a generatív mesterséges intelligencián alapuló kódgenerátorok által támogatott fejlesztési megoldások számára \parencite{Gartner2023LCNC}.

\section{Mesterséges intelligencia}\label{sec:mi}

A \textit{mesterséges intelligencia} (MI, artificial intelligence, AI) fogalma eltér a köznyelvben és a tudományos diskurzusban. A hétköznapi kommunikációban az MI gyakran bármilyen automatizált vagy „okos” működésű rendszert jelöl – például egy alkalmazást, egy chatbotot vagy egy autóban működő navigációs rendszert. A tudományos definíciók ezzel szemben szűkebbek és pontosabbak: mesterséges intelligenciának azokat a számítógépes rendszereket tekintjük, amelyek képesek olyan kognitív funkciókat utánzó műveletekre, mint az érvelés, a tanulás, az érzékelés vagy a döntéshozatal \parencite{RussellNorvig2021AI}.  

A definíció tisztázatlansága régóta problémát jelent az információrendszerek kutatásában is. Egy 2005 és 2020 között megjelent cikkeket feldolgozó szisztematikus irodalomáttekintés szerint a mesterséges intelligencia kifejezést az információrendszerek szakirodalma sokszor homályosan használja, és a kutatások egy része nem tesz különbséget az MI, a gépi tanulás és az adatelemzés között \parencite{Collins2021AIinIS}. A szerzők szerint a jövőbeni kutatások egyik kulcsfeladata az MI pontos fogalmi kereteinek meghatározása, hogy az empirikus vizsgálatok jobban összehasonlíthatók legyenek.

\subsection{Történeti áttekintés}\label{subsec:mitortenelem}

A mesterséges intelligencia története szorosan összefonódik a számítástechnika fejlődésével. Az MI kifejezést először \textcite{McCarthy1955Proposal} használta a híres Dartmouth-konferencián, ahol a kutatók célul tűzték ki az „intelligens gépek” megalkotását. Az 1950-es és 1960-as években az MI kutatásokat a \textit{szimbolikus, szabályalapú rendszerek} uralták: az olyan korai projektek, mint az \textit{ELIZA} \parencite{Weizenbaum1966ELIZA} vagy a \textit{SHRDLU} \parencite{Winograd1972SHRDLU} egyszerű természetesnyelv-feldolgozási feladatokat oldottak meg szabályrendszerek segítségével.

A hetvenes évek közepétől az úgynevezett \textit{MI-tél} (AI winter) időszaka következett, mivel a technológiai korlátok és a túlzott várakozások miatt a kutatások lassultak. Az 1980-as években a \textit{szakértői rendszerek} (expert systems) jelentették az új áttörést, amelyek például az orvosi diagnosztikában vagy az ipari hibadetektálásban használták a mesterséges intelligenciát \parencite{Feigenbaum1981ExpertSystems}. A \textit{gépi tanulás} (machine learning) koncepciója ekkor kezdett erősödni, de a valódi forradalom a 2010-es években következett be, amikor a nagy mennyiségű adat (Big Data), a megnövekedett számítási teljesítmény és a grafikus processzorok (GPU-k) elérhetősége lehetővé tette a \textit{mélytanulás} (deep learning) gyakorlati alkalmazását.

A 2010-es évektől a mesterséges intelligencia a mindennapi technológiák részévé vált. A Google, az Amazon és a Microsoft felhőalapú MI-szolgáltatásokat kínál, miközben az önvezető járművek, az orvosi diagnosztikai rendszerek és a pénzügyi döntéstámogató algoritmusok is mind mesterséges intelligencián alapulnak. A ChatGPT\footnote{\url{https://chatgpt.com/}} és más \textit{nagy nyelvi modellek} (Large Language Models, LLM) megjelenése a 2020-as évek elején újabb paradigmaváltást hozott, amelyet már sokan a \textit{generatív mesterséges intelligencia} korszakának neveznek \parencite{Cao2023AIGC}. Ezt a fejlődési ívet ma az \textit{agentic AI} irányzat folytatja, amelynek célja, hogy a mesterséges intelligencia ne csupán tartalmat generáljon, hanem autonóm módon képes legyen döntéseket hozni és tevékenységeket végrehajtani \parencite{AgenticAI}.

\subsection{Főbb irányok}\label{subsec:mifobbiranyok}

A mesterséges intelligencia története során több irányzat különíthető el, amelyek részben egymást részben kiegészítve, részben felváltva dominálták a kutatást és a gyakorlati alkalmazásokat.

\paragraph{Szimbolikus mesterséges intelligencia (Good Old-Fashioned AI).}
A szimbolikus MI, más néven „klasszikus” vagy „Good Old-Fashioned AI” (GOFAI), logikai szabályok és explicit tudásreprezentációk alapján működik. A rendszerek előre definiált tudásbázisokra és következtetési szabályokra épülnek. A megközelítés előnye, hogy a döntéshozatal átlátható és magyarázható, hátránya viszont a korlátozott tanulási képesség és a nagymértékű manuális tudásbevitel igénye \parencite{NewellSimon1976PhysicalSymbolSystem}.  

\paragraph{Gépi tanulás (Machine Learning).}
A gépi tanulás a mesterséges intelligencia egyik legfontosabb alterülete, amelyben a rendszerek adatmintákból, nem pedig előre programozott szabályokból tanulnak. Ezzel a módszerrel kiváltható, hogy embereknek kelljen explicit leírni komplex szabályrendszereket, ehelyett az MI ,,magától'' képes megtanulni az adatok mögötti összefüggéseket. A gépi tanulás három fő kategóriája: a felügyelt (supervised), a nem felügyelt (unsupervised) és a megerősítéses (reinforcement) tanulás. A leggyakrabban alkalmazott technikák közé tartoznak a döntési fák, a támogatott vektorgépek (Support Vector Machine, SVM), a neurális hálók és a Bayes-féle modellek \parencite{Mitchell1997ML}.

\paragraph{Mélytanulás (Deep Learning).}
A mélytanulás a neurális hálózatok többrétegű architektúráira épül, amelyek képesek hierarchikus jellemzők automatikus kinyerésére az adatokból. Ezzel a módszerrel jelentős áttörést sikerült elérni a gépi látás, a beszédfelismerés és a természetesnyelv-feldolgozás (natural Language Processing, NLP) területén. A mélytanulás különösen hatékony, ha nagy mennyiségű címkézett adat és nagy számítási kapacitás áll rendelkezésre \parencite{LeCun2015DeepLearning}.

\paragraph{Generatív mesterséges intelligencia.}
A generatív MI (GMI) az utóbbi évek legjelentősebb innovációs irányzata, amely képes új tartalmak – például szöveg, kép, zene vagy programkód – előállítására a tanulási adatok mintázatai alapján. A generatív modellek legismertebb típusai a generatív adverszárius hálók (GAN), a variációs autoenkóderek (Variational Autoencoder, VAE) és a transzformer alapú nagy nyelvi modellek (Large Language Model, LLM) \parencite{BanhStrobel2023GenerativeAI,Cao2023AIGC}.
Ezek az architektúrák nemcsak mintákat ismernek fel, hanem képesek új, koherens és kontextushoz illeszkedő kimeneteket előállítani. A legmodernebb modellek már több százmilliárd paraméterrel rendelkeznek, és bizonyos feladatokban képesek megközelíteni az emberi szintű teljesítményt \parencite{Hadi2024LLMReview}.

\paragraph{Agentikus mesterséges intelligencia.}
Az agentikus MI (agentic AI) egy feltörekvő paradigma, amelyben az MI-t már nem is puszta tartalom-előállításra (mint a generatív MI-t), hanem autonóm cselekvésre használják. Az MI ágensek előre definiált feladatokat teljesítenek önállóan (vagy minimális emberi felügyelettel). Működésük alapja a tervezési (planning) és következtetési (reasoning) képesség, amelyhez gyakran nagy nyelvi modelleket (LLM) használnak. Az agentikus rendszerek képesek egy komplex feladatot lépésekre bontani, külső eszközöket használni információgyűjtésre vagy műveletek végrehajtására, és a visszajelzésekből tanulva dinamikusan adaptálódni a változó körülményekhez \parencite{AgenticAI}. Ez a megközelítés megkísérli még közelebb vinni az MI rendszerek működését az emberi cselekvéshez: az MI reaktív eszközből proaktív szereplővé válik.

\subsection{Erősségek, korlátok és kockázatok}\label{subsec:mikorl}

A mesterséges intelligencia legnagyobb ereje az adatokból való tanulás képessége: az MI-rendszerek hatalmas adathalmazokban is képesek felfedezni rejtett mintázatokat, amelyek emberi elemzők számára láthatatlanok maradnának. Ugyanakkor az MI megbízhatósága nagymértékben függ az adatok mennyiségétől és minőségétől – ha az adathalmazok torzítottak, az algoritmusok is torz eredményeket fognak produkálni \parencite{Mehrabi2021BiasFairness}.

A technológia másik komoly korlátja az \textit{állapottér-robbanás} (state space explosion), ami azt jelenti, hogy a lehetséges megoldási kombinációk száma exponenciálisan növekszik a bemeneti paraméterek számával. Emiatt a legtöbb MI-modell nem képes garantálni a globálisan optimális megoldást, hanem heurisztikus, közelítő stratégiákat alkalmaz, lokális optimumokat adva eredményül. A mély neurális hálók további nehézsége a tesztelhetőség és a helyességbizonyítás kérdése: mivel a döntéshozatal a belső súlyok millióin alapul, a rendszerek gyakorlatilag „feketedobozként” működnek, azaz nem tudhatjuk pontosan, miért az adott eredményt adja \parencite{Raji2022TestingAI}.

Az MI bevezetése adatvédelmi és etikai kockázatokat is hordoz. A nagy nyelvi modellek például képesek lehetnek privát információk tárolására vagy visszaidézésére, ami adatszivárgási veszélyt jelenthet. További problémát okoz, hogy az algoritmusok gyakran nem adnak lehetőséget a felhasználói kontrollra vagy az „értesítés és választás” alapelv érvényesítésére, ami adatvédelmi jogi aggályokat vet fel \parencite{Plant2022Privacy}.  

A vállalati környezetben különösen fontos a \textit{responsible AI} (felelős mesterséges intelligencia) elveinek betartása, ideértve a modellek átláthatóságát, auditálhatóságát és az emberi döntéshozatal fenntartását kritikus folyamatokban \parencite{Floridi2019Unified}. 
Noha a mesterséges intelligencia rendszerek képesek önálló döntéseket hozni és javaslatokat generálni, a felelősség végső soron mindig az emberi felhasználót terheli. A vállalatoknak ezért egyértelmű felelősségi kereteket kell kialakítaniuk az MI-alkalmazások használatakor, hogy világos legyen, ki viseli a következményeket egy hibás döntés, félrevezető javaslat vagy adatkezelési incidens esetén. Az MI tehát nem váltja ki az emberi felelősséget, csak a formáját változtatja meg: a technológiai döntések etikai és jogi következményeit továbbra is az emberi szereplőknek kell viselniük.

Az MI hosszú távú hatásait tekintve több kutató felveti az emberi kompetenciák fokozatos eróziójának kockázatát is. Ha a szakemberek mindennapi munkájukban egyre inkább az MI-re támaszkodnak, fennáll a veszélye, hogy a döntéshozatal és a problémamegoldás képessége fokozatosan csökken \parencite{soc15010006}. Ugyanakkor az MI nem az ember helyettesítője, hanem eszköze lehet: a legnagyobb potenciál a hibrid rendszerekben rejlik, ahol az emberi intuíció és a mesterséges analitika egymást erősítve működik együtt \parencite{HumanAI}.

\section{Mesterséges intelligencia a szoftverfejlesztésben}\label{sec:mi-fejl}

A szoftverfejlesztés komplex mérnöki feladat, ahol a siker nemcsak technikai tudáson, hanem kreativitáson, együttműködésen és folyamatos tanuláson is múlik. Mivel a fejlesztés specifikus szaktudást igénylő tevékenység, költségei is magasak, így a hatékonyság növelésének kérdése gyakorlatilag egyidős magával a szoftverfejlesztéssel. A különböző fejlesztési módszertanok, programnyelvek, integrált fejlesztői környezetek és automatizációs megoldások mind ugyanazt a célt szolgálták: rövidebb idő alatt, vagyis hatékonyabban és alacsonyabb költségen lehessen egyre komplexebb szoftvereket létrehozni, egyre magasabb minőségben.

A mesterséges intelligencia megjelenése új fejezetet nyitott ebben a folyamatban. Az MI széleskörű alkalmazhatósága miatt gyakorlatilag a szoftverfejlesztési életciklus minden szakaszában képes értéket teremteni, a követelményfeltárástól kezdve a kódoláson, tesztelésen és dokumentáláson át egészen az üzemeltetésig \parencite{IBM2025AIinSD}. Az MI-alapú eszközök ma már nemcsak kiegészítő segédeszközök, hanem a fejlesztési folyamat integrált (sőt, gyakran elengedhetetlen) részei, amelyek az emberi tudást és tapasztalatot kiegészítve támogatják a fejlesztést.

\subsection{Nemgeneratív mesterséges intelligencia a szoftverfejlesztésben}

A mesterséges intelligencia már a generatív modellek megjelenése előtt is fontos szerepet játszott a szoftverfejlesztésben. Már a 2000-es évek elejétől kezdve alkalmaztak gépi tanulási és adatbányászati módszereket a hibák előrejelzésére, a kódminőség javítására, valamint a tesztelés és üzemeltetés automatizálására. Ezeket a megoldásokat nem tekintjük „generatívnak”, hiszen nem új kódot vagy szöveget hoznak létre, hanem elemzéssel, előrejelzéssel vagy döntéstámogatással támogatják a fejlesztési folyamatot.

Ez az alfejezet csupán néhány jellegzetes felhasználási módot mutat be példaként az SDLC különböző fázisaiból, ez a felsorolás azonban semmiképpen sem tekinthető szisztematikus áttekintésnek. A bemutatott felhasználási módokat a \ref{tab:nemgenai-sdlc-tools}. táblázat foglalja össze.

\paragraph{Fejlesztési időtartam előrejelzése.}
A projektmenedzsment folyamatokban az MI-t gyakran használják becslésre.  
A gépi tanulási modellek képesek korábbi projektadatokból (pl. story pointok, commit mennyiség, csapatteljesítmény) tanulni, és ezek alapján előrejelezni a jövőbeli feladatok időigényét \parencite{SwEffortPrediction}.

\paragraph{Erőforrás-allokáció optimalizálása.}
Az erőforrás-allokációs problémák — például hogy melyik fejlesztők, melyik feladatokon, milyen sorrendben dolgozzanak — jól formalizálhatók optimalizációs problémaként.  
A modern MI-eszközök prediktív modelleket alkalmaznak a csapatteljesítmény és a feladatbonyolultság elemzésére. Az ilyen döntéstámogató rendszerek alkalmasak a menedzsment folyamatok támogatására, hosszabb távon akár automatizálására is \parencite{ResourceAlloc}.

\paragraph{Követelményellenőrzés.}
Mivel a követelményekre épül később a teljes szoftverfejlesztési folyamat, kiemelt fontosságú, hogy precízen legyenek megfogalmazva. A természetesnyelv-feldolgozás (NLP) segítségével a szöveges követelmények is elemezhetők, és ellenőrizhető, hogy teljesítik-e a követelményekkel szemben támasztott általános elvárásokat, mint pl. egyértelműség, teljesség és ellentmondásmentesség \parencite{NLP4RE}.

\paragraph{Architektúraelemzés.}
A szoftverarchitektúra alapvetően meghatározza a rendszer hosszú távú karbantarthatóságát. Egyes MI-eszközök képesek a rendszer forráskódjából és függőségi gráfjából automatikusan következtetni bizonyos architekturális tulajdonságokra, mint például a rétegsértések, ciklikus függőségek, instabil komponensek vagy más úgynevezett \emph{architecture smell}-ek (hibára utaló mintázatok) jelenléte. Az ilyen eszközök statisztikai és gépi tanulási módszerekkel azonosítják a tipikus mintázatokat \parencite{ArchSmell}.

\paragraph{Statikus kódanalízis.}
A statikus kódanalízis célja, hogy a forráskódot futtatás nélkül vizsgálja, és azonosítsa a hibalehetőségeket, kódstílusbeli problémákat vagy biztonsági sebezhetőségeket.  
A modern eszközök a hagyományos szintaktikai ellenőrzésen túl már gépi tanulási modelleket is használnak a hibák mintázatainak felismerésére. Az ilyen modellek korábbi hibajavítási adatokat használnak tanításhoz, és képesek a fejlesztői szokásokból tanulni \parencite{Amalfitano2023AISTestingTertiary}.

\paragraph{Hibapredikció.}
A hibapredikció célja annak előrejelzése, hogy egy adott kódrészletben vagy modulban mekkora a hibák megjelenésének valószínűsége.  
Míg a statikus kódanalízis a kód állapotát vizsgálja, addig a hibapredikció időbeli adatokat (pl. commit history, fejlesztői aktivitás, hibajegyek) elemez, így képes megtalálni azokat a komponenseket, amelyek a jövőben nagyobb karbantartási kockázatot hordoznak \parencite{Yang2022BugPredictionSurvey}.

\paragraph{Tesztadat-generálás.}
Fontos különbséget tenni a kombinatorikus és a generatív megközelítések között: a nemgeneratív (kombinatorikus) modellek meglévő adatok vagy specifikációk alapján generálnak új bemeneteket — például keresési algoritmusok, genetikus optimalizáció vagy constraint-solver technikák segítségével \parencite{zhang2014automatic}.  
Ezek a rendszerek nem „találnak ki” új tesztadatokat, hanem a lehetséges bemeneti tér lefedettségét optimalizálják, növelve a tesztelés hatékonyságát.

\paragraph{Regressziósteszt-priorizálás.}
A regressziós tesztelés során a szoftver új verzióinak ellenőrzése történik, hogy a módosítás nem rontotta el azt, ami eddig már működött. Ez nagyméretű rendszerek esetén rendkívül időigényes lehet.  
A regressziós tesztprioritás célja, hogy a teszteket olyan sorrendben futtassa, amely minimalizálja az időráfordítást és maximalizálja a hibák korai detektálását. Erre azért van szükség, mert a tesztelésre rendelkezésre álló idő mindig kevesebb, mint az összes teszt teljes futási ideje \parencite{RegrTestPrio}.

\paragraph{Anomáliafelismerés.}
Az üzemeltetési és karbantartási szakaszban az anomáliafelismerés a normál működéstől eltérő viselkedés automatikus azonosítását jelenti, amire szintén használhatók MI-alapú megoldások. A rendszerlogok, teljesítménymutatók és hálózati forgalmi adatok alapján működő AIOps-rendszerek képesek proaktívan jelezni egy esetleges meghibásodást vagy teljesítményromlást \parencite{AnomalyDetectionAI}.

\begin{table}[htbp]
	\centering
	\caption{Nemgeneratív MI felhasználások és eszközök az SDLC fázisaiban}
	\label{tab:nemgenai-sdlc-tools}
	\begin{tabular}{p{3cm} p{5cm} p{6cm}}
		\toprule
		\textbf{SDLC fázis} & \textbf{Felhasználás típusa} & \textbf{Eszközök} \\
		\midrule
		
		\textbf{Projekttervezés}
		& Fejlesztési időtartam előrejelzése 
		& Atlassian Forecast\tablefootnote{\url{https://marketplace.atlassian.com/vendors/1213598/forecast}} \\
		& Erőforrás-allokáció optimalizálása 
		& Monday AI\tablefootnote{\url{https://monday.com/}}, 
		ClickUp Brain\tablefootnote{\url{https://clickup.com/brain}} \\
		
		\midrule
		
		\textbf{Elemzés} 
		& Követelményellenőrzés 
		& IBM Engineering Requirements Quality Assistant\tablefootnote{\url{https://www.ibm.com/docs/en/erqa}}, 
		ScopeMaster Requirements Analyser\tablefootnote{\url{https://www.scopemaster.com/solutions/requirements-analyser/}} \\
		
		\midrule
		
		\textbf{Rendszertervezés} 
		& Architektúraelemzés 
		& Designite\tablefootnote{\url{https://www.designite-tools.com/}}, 
		Embold\tablefootnote{\url{https://embold.io/}} \\
		
		\midrule
		
		\textbf{Fejlesztés} 
		& Statikus kódanalízis 
		& SonarQube\tablefootnote{\url{https://www.sonarsource.com/products/sonarqube/}} \\
		& Hibapredikció 
		& Bugspots\tablefootnote{\url{https://github.com/igrigorik/bugspots}} \\
		
		\midrule
		
		\textbf{Tesztelés} 
		& Tesztadat-generálás 
		& Microsoft PICT\tablefootnote{\url{https://github.com/microsoft/pict}}, 
		Hexawise\tablefootnote{\url{https://hexawise.com/}} \\
		& Regressziósteszt-priorizálás 
		&Leapwork\tablefootnote{\url{https://www.leapwork.com/}} \\
		
		\midrule
		
		\textbf{Karbantartás} 
		& Anomáliafelismerés 
		& Datadog Watchdog\tablefootnote{\url{https://www.datadoghq.com/product/platform/watchdog/}}, 
		Dynatrace Davis AI\tablefootnote{\url{https://www.dynatrace.com/platform/artificial-intelligence/}} \\
		
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Generatív mesterséges intelligencia a szoftverfejlesztésben}\label{subsec:genai-dev}

A nemgeneratív és a generatív mesterséges intelligencia közti alapvető különbség a szoftverfejlesztésbeli felhasználásukban is tetten érhető: míg előbbi az emberek által előállított különféle artefaktumokat (dokumentum, programkód, konfiguráció stb.) elemzi, utóbbi már konkrétan előállítja ezeket. Vagyis a generatív mesterséges intelligencia – emberi utasítások (promptok) alapján – képes a szoftverfejlesztési folyamat során előállítandó kimenetek közvetlen létrehozására.

A generatív MI modellek tehát tekinthetők emberi nyelven („emberi nyelvi interfészen”) irányítható fejlesztői asszisztenseknek. A kódolásban ez olyan, mint a páros programozás félig gépi megvalósítása, ahol az MI egy mindig elérhető és soha el nem fáradó „pair programmer” (jóllehet nem támogató, csak autonóm). Gyorsan képes ötleteket és megoldási javaslatokat adni, kódvázlatokat előállítani, meglévő kódokat elemezni, ugyanakkor nem tévedhetetlen, ezért emberi kontrollt igényel \parencite{LLMcodeHIL}.

A 2020-as években megjelenő nagy nyelvi modellek (LLM) alapozták meg a generatív MI ezirányú felhasználását. Az óriási mennyiségű szövegen (így többek között programkódokon is) tanított modellek képesek kontextusba illő válaszokat adni – programozási kérdésekre is. Az általános LLM-ek mellett azonban megjelentek a célspecifikus modellek is, mint pl. a kifejezetten programkódokra finomhangolt megoldások, az ún. code LLM-ek. Ezeket a modelleket az általános szövegek mellett nagy mennyiségű forráskódon, dokumentáción, pull requesten és hibajegyen tanították. Ennek eredményeként a modellek megtanulják a programkódok tipikus mintázatait, a programozási nyelvek szintaktikai szabályaitól kezdve egészen a kódok magas szintű szerkezetéig \parencite{LLMdev}.

\paragraph{Fejlesztői eszközök.}
A generatív mesterséges intelligencia integrációja rohamosan terjed a fejlesztői eszközökben \parencite{Gartner_AICodeAssistants_2025}. Ennek egyik formája az \textit{integrált fejlesztői környezetekbe} (integrated development environment, IDE) épített kódolási asszisztens. Ezek az eszközök nemcsak egyszerű kódkiegészítést biztosítanak (mint már a nemgeneratív eszközök is), hanem egész sorokat, sőt, teljes függvényeket vagy osztályokat is legenerálnak. A modern eszközök képesek figyelembe venni a kód kontextusát is: átlátják a szerkesztett fájlt vagy akár a teljes projektet, és ennek megfelelő javaslatokat adnak.

A másik jellemző eszköz a chat alapú fejlesztői asszisztens, amely lehetővé teszi, hogy a fejlesztő párbeszédet folytasson az MI modellel a kódról. Ez használható bonyolult függvények magyarázatára, alternatív megoldások keresésére, hibakeresésre. Az agentikus megközelítés ennél is tovább megy, azok a rendszerek már képesek közvetlenül végrehajtani a feladatokat (pl. kódot generálni, projektet refaktorálni, fordítani). %Ezek az eszközök különösen akkor hatékonyak, amikor egy már létező összetett logika vagy régi, rosszul dokumentált kódbázis felderítése és megértése a cél.

\paragraph{Hallucináció.}
A nyelvi modellek kapcsán fontos megemlíteni a \emph{hallucináció} problémáját, vagyis amikor a modell olyan választ állít elő, amely kontextusba illőnek és helyesnek tűnik, vagyis hihető, ugyanakkor mégis hibás.\footnote{Egyes szakértők felhívják a figyelmet, hogy a hallucináció elnevezés egy félrevezető eufemizmus, hiszen ez valójában az LLM-ek működésének a lényege. Amit hallucinációnak hívunk, az a gyakorlatban egyszerű tévedés \parencite{LLM-hallucinate-mistake}.} Ez egyaránt jelenthet egy téves tényadatot (pl. egy egyszerű matematikai számítás eredménye) vagy hivatkozást valamire, ami valójában nem létezik (pl. szolgáltatás, tudományos mű). A hallucináció a modellek jelenlegi tanítási és kiértékelési módszereiből ered, amelyek leegyszerűsítve a találgatást jutalmazzák a bizonytalanság beismerésével szemben \parencite{hallucinate}.

\paragraph{Kontextus.}
A modellek gyakorlati hasznosíthatósága szempontjából kulcskérdés a \emph{kontextus} mérete és kezelése. Minél komplexebb problématerekről van szó, annál inkább elengedhetetlen a modellek széles látóköre, pl. egy több ezer fájlból álló projekt esetén nagyon korlátosan használható egy olyan modell, amely csak az adott fájlt képes átlátni, hiszen egy közelről helyesnek tűnő megoldás lehet, hogy valójában több új problémát okoz, mint ahányat megold.

A kontextus tágítása azonban nemcsak mennyiségi kérdés. Nagyvállalati környezetben azok a tudáselemek, amelyek egy probléma megoldásához szükségesek, gyakran nem egy helyen és azonos formában állnak rendelkezésre, hanem széttöredezetten. Vagyis pl. egyaránt kellene értelmezni különféle szoftver projekteket, hibajegyeket, dokumentumokat, adatbázisokat és tudásmegosztó oldalakat \parencite{LLMknowledge}.

Ahhoz tehát, hogy nagyvállalati környezetben is hatékonyan tudjuk használni a generatív MI-t, kulcsfontosságú az MI és a vállalati rendszerek mély integrációja. Fontos látni, hogy ez az integráció kétirányú tudásmegosztást feltételez, és mindkét irány számos kihívást és kockázatot hordoz. Egyrészt a vállalati rendszerekben felhalmozott tudást az MI modell számára hozzáférhetővé kell tenni, hogy az képes legyen beemelni azt a saját kontextusába. Másrészt a generatív MI modellek kimeneteit úgy kell integrálni a vállalati folyamatokba, hogy ezek a megoldások ne szigetszerű eszközökként jelenjenek meg a szervezeten belül, hanem a vállalati működés szerves, beágyazott elemeivé váljanak.

\section{Iparági áttekintés}

A szoftveripar a globális gazdaság egyik leggyorsabban növekvő és legnagyobb hatású ágazata. A digitális transzformáció évtizedeiben szinte minden iparág szoftvervezéreltté vált, ami drasztikusan megnövelte a szoftverfejlesztés iránti keresletet. Az iparági elemzések évek óta folyamatos, jelentős növekedést jeleznek a vállalati szoftverpiacon \parencite{GVR_2025_SoftwareMarket}. Ezen belül is külön említést érdemelnek a felhőalapú szolgáltatások, a SaaS (Software as a Service) modellek, a kibervédelem és az üzleti folyamatok digitalizációja. A szoftveripar a kezdetekben támogató funkciót töltött be, napjainkra azonban már számos szervezet alapvető versenyképességi tényezőjévé vált. Ezen szervezetek esetében a gyors fejlesztés, a megbízhatóság és a skálázhatóság közvetlenül teremtenek üzleti értéket.

A nagyvállalati szoftverfejlesztés lényegesen eltér a kisebb projektek vagy startupok világától. Ezek a rendszerek gyakran több évtizedes múlttal, sokszor különböző generációk technológiáira épülve működnek tovább. A több millió soros monolitikus rendszerek, az üzletmenet szempontjából kritikus szoftverek (pl. core pénzügyi rendszerek, telekommunikációs platformok) és a heterogén infrastruktúrák mind növelik a komplexitást, amit emberi kapacitásokkal már egyre nehezebb teljes mértékben átlátni. A folyamatosan szaporodó compliance követelmények tovább bonyolítják a rendszereket, miközben a vállalatok változatlanul rövid fejlesztési ciklusokat és folyamatos innovációt várnak el. A rendszerek és komponenseik közötti erős kölcsönhatások miatt egy látszólag egyszerű változtatás is nagy kockázatokkal járhat, így gyakran a látszólag egyszerű fejlesztői döntések előtt sem hagyható ki az előbb felsorolt komplexitás áttekintése.

A fenti tényezők mellett az iparágban kritikus kérdés a technikai adósság (technical debt) és az örökölt (legacy) rendszerek fenntartása is. Számos szervezet informatikai infrastruktúrájának jelentős része még mindig olyan technológiákon fut, amelyeket nehéz modernizálni, mert a kritikus belső folyamatok ezekre épülnek. A legacy rendszerek frissítése gyakran magas kockázattal jár, a dokumentáció nem teljes, a tudás pedig sokszor személyekhez kötött (tribal knowledge). A minőségbiztosítás, a tesztelés és a hibajavítás így a nagyvállalatok egyik legköltségesebb és legmunkaigényesebb tevékenységévé vált \parencite{legacy}.

Mindez jól magyarázza, miért vált a generatív mesterséges intelligencia az iparág egyik legígéretesebb technológiájává. A rendszerek növekvő mérete, a tudás szétszórtsága és a dokumentáció hiánya olyan környezetet eredményez, ahol különösen nagy értéke van annak, ha egy modell képes nagy mennyiségű szöveget, kódot és információt egyszerre értelmezni. A generatív MI nem csupán végrehajtási automatizálást tesz lehetővé, hanem a várakozások szerint a fejlesztési folyamat szellemi terheit is képes csökkenteni: kódolási mintázatokat ismer fel, alternatív megoldásokat javasol, összefoglalja a kontextust, és segít átlátni a komplex rendszerek működését. Ez a képesség teszi relevánssá és stratégiai jelentőségűvé a generatív MI-t a nagyvállalati szoftverfejlesztésben, különösen olyan környezetekben, ahol a gyorsaság, a minőség és a szabályozási megfelelőség egyszerre kritikus elvárás.